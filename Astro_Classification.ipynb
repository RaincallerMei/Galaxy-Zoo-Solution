{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaincallerMei/Galaxy-Zoo-Solution/blob/main/Astro_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBEO5K031Fxw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrQ5Gr_Y3rMh",
        "outputId": "b7e1d302-6d6d-4c15-9c31-af28aa2e6257"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml97jfMw4gGc",
        "outputId": "e6a0d8b2-9b78-46d2-ca26-e89190db88c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/AstroCNN/galaxy-zoo-the-galaxy-challenge.zip\n",
            "  inflating: /content/dataset/all_ones_benchmark.zip  \n",
            "  inflating: /content/dataset/all_zeros_benchmark.zip  \n",
            "  inflating: /content/dataset/central_pixel_benchmark.zip  \n",
            "  inflating: /content/dataset/images_test_rev1.zip  \n",
            "  inflating: /content/dataset/images_training_rev1.zip  \n",
            "  inflating: /content/dataset/training_solutions_rev1.zip  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/AstroCNN/galaxy-zoo-the-galaxy-challenge.zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hERWJ8U6oG5",
        "outputId": "bf6d92fe-3581-4287-a50f-13acabc72e43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace all_zeros_benchmark/all_zeros_benchmark.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace training_solutions_rev1/training_solutions_rev1.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace images_test_rev1/images_test_rev1/100018.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace images_test_rev1/images_test_rev1/100037.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace images_test_rev1/images_test_rev1/100042.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace images_test_rev1/images_test_rev1/100052.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace images_test_rev1/images_test_rev1/100056.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "replace central_pixel_benchmark/central_pixel_benchmark.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace all_ones_benchmark/all_ones_benchmark.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "replace images_training_rev1/images_training_rev1/100008.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Go to the directory where the 6 zips live\n",
        "os.chdir(\"/content/dataset\")\n",
        "\n",
        "# Find all the .zip files\n",
        "zip_files = glob.glob(\"*.zip\")\n",
        "\n",
        "# Unzip each\n",
        "for zipf in zip_files:\n",
        "    folder_name = os.path.splitext(zipf)[0]  # e.g., \"folder1\" from \"folder1.zip\"\n",
        "    !unzip -q {zipf} -d {folder_name}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8LNZibG7VH8"
      },
      "outputs": [],
      "source": [
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR3XkskPP74M"
      },
      "source": [
        "#Match Images to CSV Rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJoOBeOoP-Ly"
      },
      "source": [
        "Load and Index the CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H55nfoC67hBq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "solutions_df = pd.read_csv(\"/content/dataset/training_solutions_rev1/training_solutions_rev1.csv\")\n",
        "\n",
        "# Suppose the columns are like:\n",
        "# GalaxyID, p1, p2, ..., p37\n",
        "# We'll create a dict { galaxy_id: [p1, p2, ..., p37], ... }\n",
        "\n",
        "id_to_probs = {}\n",
        "for row in solutions_df.itertuples(index=False):\n",
        "    galaxy_id = str(row.GalaxyID)\n",
        "    # row has p1, p2, ..., p37 as subsequent columns\n",
        "    # Convert them to a list or tensor\n",
        "    probabilities = list(row[1:])  # row[0] is GalaxyID, row[1:] are the 37 prob columns\n",
        "    id_to_probs[galaxy_id] = probabilities\n",
        "\n",
        "#Now id_to_probs[\"123456\"] might be [0.123, 0.456, ..., 0.999], etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HNobSe0QBtE"
      },
      "source": [
        "Custom Dataset Class:\n",
        "\n",
        "\n",
        "1.   Lists all image files in training/.\n",
        "2. For each file, extracts GalaxyID from the filename (e.g., 123456 from 123456.jpg).\n",
        "3. Finds the matching probabilities from id_to_probs.\n",
        "4. Loads the image, applies transformations, and returns (image_tensor, label_tensor).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PxdCBy-7qHh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class GalaxyDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 images_dir,\n",
        "                 id_to_probs,\n",
        "                 transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.id_to_probs = id_to_probs\n",
        "        self.transform = transform\n",
        "\n",
        "        # List all JPG files in images_dir\n",
        "        self.image_paths = [\n",
        "            os.path.join(images_dir, f)\n",
        "            for f in os.listdir(images_dir)\n",
        "            if f.lower().endswith('.jpg')\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        # Filename is something like \"123456.jpg\"\n",
        "        filename = os.path.basename(img_path)\n",
        "        galaxy_id = os.path.splitext(filename)[0]  # \"123456\"\n",
        "\n",
        "        # Load image\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Convert probabilities to a tensor\n",
        "        # If galaxy_id doesn't exist in id_to_probs,\n",
        "        # you might want to handle KeyError or skip it\n",
        "        probabilities = self.id_to_probs.get(galaxy_id, None)\n",
        "        if probabilities is None:\n",
        "            # If no label is found, you could handle it\n",
        "            # (e.g., raise an Exception, or return dummy data)\n",
        "            raise ValueError(f\"GalaxyID {galaxy_id} not found in id_to_probs\")\n",
        "\n",
        "        # Transform image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert list of probabilities to a FloatTensor\n",
        "        label = torch.tensor(probabilities, dtype=torch.float32)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnTEmJb3T7ir"
      },
      "source": [
        "Transform the jpg/PIL images into tensors!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5hYwvD6UAG-"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "transformImage = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.ToTensor(),\n",
        "    # Possibly T.Normalize(mean, std) if needed\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ7NNUQZQKxh"
      },
      "source": [
        "#4. Creating a DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF0cUXLmQNDI"
      },
      "source": [
        "Create a Dataset and a DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xck_nYiVQNO4"
      },
      "outputs": [],
      "source": [
        "train_dir = \"/content/dataset/images_training_rev1/images_training_rev1\"\n",
        "train_dataset = GalaxyDataset(\n",
        "    images_dir=train_dir,\n",
        "    id_to_probs=id_to_probs,\n",
        "    transform=transformImage\n",
        ")\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=2  # or more, depending on environment\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cTCP6YoQphF"
      },
      "source": [
        "#5. Defining the CNN Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fditA0GPQrvA"
      },
      "source": [
        "GOATED CNN MODEL IMPLEMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKcb57x-QxWX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.v2 as v2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "\n",
        "class GalaxyCNN(nn.Module):\n",
        "    def __init__(self, num_outputs=37):\n",
        "        super(GalaxyCNN, self).__init__()\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Max-pooling layers\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Reduce spatial dimensions by half\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 8 * 8, 256)  # Adjusted based on 69x69 input\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, num_outputs)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Convolutional layers with ReLU activation and pooling\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # Output shape: [batch, 32, 34, 34]\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # Output shape: [batch, 64, 17, 17]\n",
        "        x = self.pool(F.relu(self.conv3(x)))  # Output shape: [batch, 128, 6, 6]\n",
        "\n",
        "        # Flatten the feature maps\n",
        "        x = x.view(x.size(0), -1)  # Output shape: [batch, 128 * 8 * 8]\n",
        "\n",
        "        # Fully connected layers with dropout\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)  # Output shape: [batch, 37]\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example preprocessing function for rotation augmentation\n",
        "# def augment_rotations(images, num_rotations=4):\n",
        "#     \"\"\"Generates multiple rotated versions of the input images.\"\"\"\n",
        "#     rotations = []\n",
        "#     for i in range(num_rotations):\n",
        "#         angle = i * (360 // num_rotations)  # Compute rotation angle\n",
        "#         rotated_images = torch.rot90(images, k=i, dims=(-2, -1))  # Rotate images\n",
        "#         rotations.append(rotated_images)\n",
        "#     return torch.cat(rotations, dim=0)  # Combine rotated views into one batch\n",
        "\n",
        "\n",
        "# Preprocessing function for cropping and downsampling\n",
        "def preprocess_images(images):\n",
        "    transform = v2.Compose([\n",
        "        v2.ToPILImage(),\n",
        "        v2.CenterCrop((207, 207)),\n",
        "        v2.Resize((69, 69)),\n",
        "        v2.ToTensor()\n",
        "    ])\n",
        "    processed_images = torch.stack([transform(image) for image in images])  # Process each image separately\n",
        "    return processed_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0-W0jkEQyuQ"
      },
      "source": [
        "#6. Training: Loss Function and Optimizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4_sSoCSQ1gq"
      },
      "source": [
        "BCEWithLogitsLoss is typically used for multi-label classification of the 37 features that can be present (probability close to 1) or absent (close to 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "e-sf0DaFRGIA",
        "outputId": "326bd441-9526-4232-a766-682f28168b6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/3849 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n",
            "Epoch 1:   0%|          | 0/3849 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (16x8192 and 100352x256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-991c498ee150>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0b0673e92649>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Fully connected layers with dropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x8192 and 100352x256)"
          ]
        }
      ],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 5\n",
        "print_every = 100\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "\n",
        "    for i, (images, labels) in enumerate(progress_bar):\n",
        "        # Preprocess images\n",
        "\n",
        "        transforms = v2.Compose([\n",
        "            v2.RandomResizedCrop(size=(207, 207), antialias=True),\n",
        "            v2.RandomHorizontalFlip(p=0.5),\n",
        "            v2.ToDtype(torch.float32, scale=True),\n",
        "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        images = transforms(images)\n",
        "\n",
        "        images = preprocess_images(images)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if i % print_every == 0:\n",
        "            avg_loss = running_loss / print_every\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i}] loss: {avg_loss:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "        # Update the progress bar with the current loss\n",
        "        progress_bar.set_postfix(loss=(running_loss/(i+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bUf8WpEWNNl"
      },
      "source": [
        "#7. Generating Predictions for Testing Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiZewHqFWOFd"
      },
      "source": [
        "After training is done:\n",
        "\n",
        "1. Create a new dataset for your testing/ folder (no labels needed).\n",
        "2. Forward pass each test image to get predicted probabilities.\n",
        "3. Save them in the same format as your training CSV or the benchmark CSVs (GalaxyID + 37 columns).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IxLN276dyuH"
      },
      "outputs": [],
      "source": [
        "class GalaxyTestDataset(Dataset):\n",
        "    def __init__(self, images_dir, transform=None):\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [\n",
        "            os.path.join(images_dir, f)\n",
        "            for f in os.listdir(images_dir)\n",
        "            if f.lower().endswith('.jpg')\n",
        "        ]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        filename = os.path.basename(path)\n",
        "        galaxy_id = os.path.splitext(filename)[0]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, galaxy_id\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_1pznW_eKZq"
      },
      "source": [
        "Generate Predictions python Copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbL9-3qseMST"
      },
      "outputs": [],
      "source": [
        "test_dir = \"/content/dataset/images_test_rev1/images_test_rev1\"\n",
        "test_dataset = GalaxyTestDataset(test_dir, transform=transformImage)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, galaxy_ids in test_loader:\n",
        "        outputs = model(images)        # shape [batch, 37]\n",
        "        probs = torch.sigmoid(outputs) # convert logits -> probabilities, shape [batch, 37]\n",
        "\n",
        "        # Iterate and store results\n",
        "        for gid, prob_vec in zip(galaxy_ids, probs):\n",
        "            # Convert prob_vec to a Python list\n",
        "            prob_list = prob_vec.cpu().numpy().tolist()\n",
        "            predictions.append((gid, prob_list))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8eiN6l0ebue"
      },
      "source": [
        "Save results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqeCz6DDecp3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "output_csv = \"/content/my_predictions.csv\"\n",
        "header = [\"GalaxyID\"] + [f\"p{i}\" for i in range(1, 38)]  # 37 prob columns\n",
        "\n",
        "with open(output_csv, mode=\"w\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    for (gid, prob_list) in predictions:\n",
        "        row = [gid] + prob_list\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(\"Predictions saved to:\", output_csv)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Test Correctness (Square Error)"
      ],
      "metadata": {
        "id": "iDuJUmb-6ljV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Loss Root Mean Squared Error:\n",
        "2. AOC"
      ],
      "metadata": {
        "id": "tZUMRf1f64tp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-zunegqE6uLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Uh Sigma\n"
      ],
      "metadata": {
        "id": "sPMrjOF-6uyy"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPZVKZwZ/JCuLTH1B5Cpd80",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}